from __future__ import annotations

import importlib
import importlib.util
import sys
from copy import deepcopy
from pathlib import Path
from typing import Any


# When executed as a module: python -m examples.vlm_multi_turn.rollout
from slime.rollout.sglang_rollout import GenerateState
from slime.utils.http_utils import post
from slime.utils.processing_utils import encode_image_for_rollout_engine
from slime.utils.types import Sample


DEFAULT_ENV_MODULE = "examples.vlm_multi_turn.env_deepeyes"
DEFAULT_ROLLOUT_CONFIG = {
    "max_turns": 20,
}


def _load_env_module(env_path: str | None):
    """Load the interaction environment module from a module path or a file path."""
    target = env_path or DEFAULT_ENV_MODULE
    module_path = Path(target)
    if module_path.suffix == ".py" and module_path.exists():
        spec = importlib.util.spec_from_file_location(f"rollout_env_{module_path.stem}", module_path)
        if spec is None or spec.loader is None:
            raise ImportError(f"Cannot import environment module from {module_path}")
        module = importlib.util.module_from_spec(spec)
        sys.modules[spec.name] = module
        spec.loader.exec_module(module)
        return module
    return importlib.import_module(target)


def _resolve_rollout_config(args: Any, env_module) -> dict[str, Any]:
    """Combine rollout defaults with optional overrides from args."""
    cfg = deepcopy(getattr(env_module, "DEFAULT_ROLLOUT_CONFIG", DEFAULT_ROLLOUT_CONFIG))
    if getattr(args, "max_turns", None) is not None:
        cfg["max_turns"] = args.max_turns
    return cfg


def _build_env(env_module, sample: Sample, args: Any):
    """Instantiate the interaction environment using the provided module."""
    build_fn = getattr(env_module, "build_env", None) or getattr(env_module, "create_env", None)
    if not callable(build_fn):
        raise ValueError("Environment module must expose a callable `build_env(sample, args)`.")
    try:
        return build_fn(sample=sample, args=args)
    except TypeError:
        # Fallback to positional signature
        return build_fn(sample, args)


def _format_observation(env_module, observation: dict) -> dict:
    """Convert an environment observation into a chat message."""
    formatter = getattr(env_module, "format_observation", None)
    if callable(formatter):
        return formatter(observation)

    observation = observation or {}
    content = []
    multimodal = observation.get("multi_modal_data") or {}
    for _, images in multimodal.items():
        for image in images:
            content.append({"type": "image", "image": image})
    content.append({"type": "text", "text": observation.get("obs_str", "")})
    return {"role": "user", "content": content}


def _merge_metadata(sample: Sample, updates: dict | None):
    if not updates:
        return
    sample.metadata = sample.metadata or {}
    for key, value in updates.items():
        if key in sample.metadata and isinstance(sample.metadata[key], dict) and isinstance(value, dict):
            sample.metadata[key] = {**sample.metadata[key], **value}
        else:
            sample.metadata[key] = value


def _handle_reset(env_module, env, observation: dict, sample: Sample, reset_info: dict | None):
    on_reset = getattr(env_module, "on_reset", None)
    if callable(on_reset):
        updates = on_reset(env=env, observation=observation, sample=sample, reset_info=reset_info)
        _merge_metadata(sample, updates)


def _finalize_episode(
    env_module,
    env,
    observation: dict,
    sample: Sample,
    responses: list[str],
) -> dict | None:
    finalize_fn = getattr(env_module, "finalize_episode", None)
    if callable(finalize_fn):
        result = finalize_fn(
            env=env,
            observation=observation,
            sample=sample,
            responses=responses,
        )
        updates = result or {}
        updates.setdefault("turns", len(responses))
        return updates
    return {}


def _encode_for_generation(
    tokenizer,
    processor,
    messages: list[dict],
    metadata: dict | None,
    apply_chat_template: bool,
    apply_chat_template_kwargs: dict | None,
):
    """
    Encode the conversation for SGLang generation (with generation prompt) and return payload pieces.
    """
    from slime.utils.processing_utils import prepare_model_inputs

    prompt_ids, extra_info = prepare_model_inputs(
        messages,
        tokenizer,
        processor,
        metadata,
        apply_chat_template,
        apply_chat_template_kwargs,
    )

    image_data = [encode_image_for_rollout_engine(img) for img in extra_info.get("images", [])]
    return prompt_ids, image_data, extra_info.get("multimodal_inputs")


async def generate(args: Any, sample: Sample, sampling_params) -> Sample:
    """Custom multi-turn rollout that interacts with a pluggable environment."""
    assert not args.partial_rollout, "Partial rollout is not supported for interaction rollouts."

    env_module = _load_env_module(getattr(args, "rollout_interaction_env_path", None))
    rollout_config = _resolve_rollout_config(args, env_module)

    state = GenerateState(args)
    tokenizer = state.tokenizer
    processor = state.processor
    url = f"http://{args.sglang_router_ip}:{args.sglang_router_port}/generate"
    sampling_params = sampling_params.copy()

    sample.metadata = sample.metadata or {}
    max_turns = rollout_config["max_turns"]
    env = _build_env(env_module, sample, args)
    try:
        observation, reset_info = env.reset()
        _handle_reset(env_module, env, observation, sample, reset_info)

        # Use the preloaded prompt (contains system + first image) as the initial conversation state
        messages = deepcopy(sample.prompt)

        prompt_ids, image_data, multimodal_inputs = _encode_for_generation(
            tokenizer,
            processor,
            messages,
            sample.metadata,
            getattr(args, "apply_chat_template", False),
            args.apply_chat_template_kwargs,
        )

        #sample.rollout_response_length is length of response produced by the actor,
        #excluding initial prompt tokens and env feedback tokens.
        sample.rollout_response_length = sample.rollout_response_length or 0
        #check for resumed sample
        if len(sample.response) > 0:
             if "max_new_tokens" in sampling_params and sampling_params["max_new_tokens"] is not None:
                sampling_params["max_new_tokens"] -= sample.rollout_response_length

        assert (
            sampling_params["max_new_tokens"] >= 0
        ), f"max_new_tokens: {sampling_params['max_new_tokens']} should not be less than 0"
        if sampling_params["max_new_tokens"] == 0:
            sample.status = Sample.Status.TRUNCATED
            return sample

        # Initialize token/logprob/loss_mask tracking to be perfectly aligned with model inputs
        if not sample.tokens:
            sample.tokens = list(prompt_ids)
        response_tokens: list[int] = sample.tokens[len(prompt_ids) :] if len(sample.tokens) >= len(prompt_ids) else []
        sample.loss_mask = sample.loss_mask or []
        sample.rollout_log_probs = sample.rollout_log_probs or []
        sample.multimodal_inputs = multimodal_inputs if sample.multimodal_inputs is None else sample.multimodal_inputs
        sample.response_length = len(response_tokens)
        current_image_data = image_data

        generated_responses: list[str] = []

        for turn_idx in range(max_turns):
            cur_sampling_params = sampling_params.copy()

            payload = {
                "input_ids": sample.tokens,
                "sampling_params": cur_sampling_params,
                "return_logprob": True,
            }

            if current_image_data:
                payload["image_data"] = current_image_data

            output = await post(url, payload)

            response_text = output["text"]
            if "output_token_logprobs" in output["meta_info"]:
                new_response_tokens = [item[1] for item in output["meta_info"]["output_token_logprobs"]]
                new_response_log_probs = [item[0] for item in output["meta_info"]["output_token_logprobs"]]
            else:
                new_response_tokens = tokenizer(response_text, add_special_tokens=False)["input_ids"]
                new_response_log_probs = [0.0] * len(new_response_tokens)

            # Append assistant response tokens/logprobs/masks
            sample.tokens.extend(new_response_tokens)
            response_tokens.extend(new_response_tokens)
            sample.loss_mask.extend([1] * len(new_response_tokens))
            sample.rollout_log_probs.extend(new_response_log_probs)
            sample.rollout_response_length += len(new_response_tokens)
            sample.response_length = len(response_tokens)

            messages.append({"role": "assistant", "content": response_text})
            generated_responses.append(response_text)

            ##update sample.response must >0 
            if len(sample.response) > 0:
                if "max_new_tokens" in sampling_params and sampling_params["max_new_tokens"] is not None:
                    sampling_params["max_new_tokens"] -= sample.rollout_response_length


            assert (
                sampling_params["max_new_tokens"] >= 0
            ), f"max_new_tokens: {sampling_params['max_new_tokens']} should not be less than 0"
            if sampling_params["max_new_tokens"] == 0:
                sample.status = Sample.Status.TRUNCATED
                return sample

            # observation, done, step_info = env.step(response_text)
            # 1. first postprocess response
            cur_response = postprocess_responses(response_text)

            # 2. execute tool（if exist tool_call）
            tool_obs, _ = await execute_predictions(cur_response)

            # 3. env judge whether Answer
            observation, done, step_info = env.step(cur_response)

            # 4. if tool has output, append output
            if tool_obs:
                obs_token_ids = tokenizer(tool_obs, add_special_tokens=False)["input_ids"]
                sample.tokens.extend(obs_token_ids)
                response_tokens.extend(obs_token_ids)
                sample.loss_mask.extend([0] * len(obs_token_ids))
                sample.rollout_log_probs.extend([0.0] * len(obs_token_ids))
            step_record = {"turn": turn_idx, "info": step_info}
            sample.metadata.setdefault("trajectory", []).append(step_record)

            if done:
                sample.status = Sample.Status.COMPLETED
                break

            # Combine previous action text with the new observation image for the next user turn
            next_user_message = _format_observation(env_module, observation)
            messages.append(next_user_message)

            # Encode only the new observation turn and append its tokens.
            obs_prompt_ids, obs_image_data, obs_multimodal_inputs = _encode_for_generation(
                tokenizer,
                processor,
                [next_user_message],
                sample.metadata,
                getattr(args, "apply_chat_template", False),
                args.apply_chat_template_kwargs,
            )

            # Drop a leading BOS if present to avoid injecting it mid-stream.
            bos_id = getattr(tokenizer, "bos_token_id", None)
            if bos_id is not None and obs_prompt_ids and obs_prompt_ids[0] == bos_id:
                obs_prompt_ids = obs_prompt_ids[1:]

            sample.tokens.extend(obs_prompt_ids)
            response_tokens.extend(obs_prompt_ids)
            sample.loss_mask.extend([0] * len(obs_prompt_ids))  # user/obs + next assistant prefix => masked
            sample.rollout_log_probs.extend([0.0] * len(obs_prompt_ids))  # keep logprob aligned with loss_mask
            sample.response_length = len(response_tokens)

            if obs_image_data:
                current_image_data = (current_image_data or []) + obs_image_data

            if obs_multimodal_inputs:
                if not sample.multimodal_inputs:
                    sample.multimodal_inputs = obs_multimodal_inputs
                elif isinstance(sample.multimodal_inputs, dict) and isinstance(obs_multimodal_inputs, dict):
                    for key, val in obs_multimodal_inputs.items():
                        if (
                            key in sample.multimodal_inputs
                            and isinstance(sample.multimodal_inputs[key], list)
                            and isinstance(val, list)
                        ):
                            sample.multimodal_inputs[key].extend(val)
                        elif (
                            key in sample.multimodal_inputs
                            and isinstance(sample.multimodal_inputs[key], dict)
                            and isinstance(val, dict)
                        ):
                            sample.multimodal_inputs[key] = {**sample.multimodal_inputs[key], **val}
                        else:
                            sample.multimodal_inputs[key] = val
                else:
                    sample.multimodal_inputs = obs_multimodal_inputs



            # check Answer
            if done:  
                sample.status = Sample.Status.COMPLETED
                break
            #multi-turn
            if turn_idx + 1 >= max_turns:
                sample.status = Sample.Status.TRUNCATED
                break


            finish_type = output["meta_info"]["finish_reason"]["type"]
            match finish_type:
                case "length":
                    sample.status = Sample.Status.TRUNCATED
                    break
                case "abort":
                    sample.status = Sample.Status.ABORTED
                    break





        # Decode only the response segment (everything after the initial prompt)
        metadata_updates = _finalize_episode(
            env_module,
            env,
            observation,
            sample=sample,
            responses=generated_responses,
        )
        sample.response = tokenizer.decode(response_tokens, skip_special_tokens=False)
        sample.response_length = len(response_tokens)
        _merge_metadata(sample, metadata_updates)

        if sample.status is None:
            sample.status = Sample.Status.COMPLETED
        return sample
    finally:
        try:
            env.close()
        except Exception:
            pass